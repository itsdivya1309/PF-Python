{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb3964d6-d8d1-4c92-b7ca-3f46ce9ad270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from typing import Union, Type\n",
    "from numba import njit\n",
    "\n",
    "from aeon.classification.base import BaseClassifier\n",
    "from aeon.distances import distance\n",
    "\n",
    "\n",
    "class Node:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_id: int,\n",
    "        _is_leaf: bool,\n",
    "        label=None,\n",
    "        class_distribution=None,\n",
    "        splitter=None,\n",
    "    ):\n",
    "        self.node_id = node_id\n",
    "        self._is_leaf = _is_leaf\n",
    "        self.label = label\n",
    "        self.splitter = splitter\n",
    "        self.class_distribution = class_distribution or {}\n",
    "        self.children = {}\n",
    "\n",
    "\n",
    "class ProximityTree(BaseClassifier):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_splitters: int = 5,\n",
    "        max_depth: int = None,\n",
    "        min_samples_split: int = 2,\n",
    "        random_state: Union[int, Type[np.random.RandomState], None] = None,\n",
    "        n_jobs: int = 1,\n",
    "        verbose: int = 0,\n",
    "    ) -> None:\n",
    "        self.n_splitters = n_splitters\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.rng = check_random_state(random_state)\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        super().__init__()\n",
    "\n",
    "    def get_parameter_value(self, X):\n",
    "        \"\"\"Generate random parameter values.\n",
    "\n",
    "        For a list of distance measures, generate a dictionary\n",
    "        of parameterized distances.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray of shape (n_cases, n_timepoints)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        distance_param : a dictionary of distances and their\n",
    "        parameters.\n",
    "        \"\"\"\n",
    "        X_std = X.std()\n",
    "        param_ranges = {\n",
    "            \"euclidean\": {},\n",
    "            \"dtw\": {\"window\": (0, 0.25)},\n",
    "            \"ddtw\": {\"window\": (0, 0.25)},\n",
    "            \"wdtw\": {\"g\": (0, 1)},\n",
    "            \"wddtw\": {\"g\": (0, 1)},\n",
    "            \"erp\": {\"g\": (X_std / 5, X_std)},\n",
    "            \"lcss\": {\"epsilon\": (X_std / 5, X_std), \"window\": (0, 0.25)},\n",
    "        }\n",
    "        random_params = {}\n",
    "        for measure, ranges in param_ranges.items():\n",
    "            random_params[measure] = {\n",
    "                param: np.round(self.rng.uniform(low, high), 3)\n",
    "                for param, (low, high) in ranges.items()\n",
    "            }\n",
    "        # For TWE\n",
    "        lmbda = self.rng.randint(0, 9)\n",
    "        exponent_range = np.arange(1, 6)  # Exponents from -5 to 1 (inclusive)\n",
    "        random_exponent = self.rng.choice(exponent_range)\n",
    "        nu = 1 / 10**random_exponent\n",
    "        random_params[\"twe\"] = {\"lmbda\": lmbda, \"nu\": nu}\n",
    "\n",
    "        # For MSM\n",
    "        base = 10\n",
    "        # Exponents from -2 to 2 (inclusive)\n",
    "        exponents = np.arange(-2, 3, dtype=np.float64)\n",
    "        # Randomly select an index from the exponent range\n",
    "        random_index = self.rng.randint(0, len(exponents))\n",
    "        c = base ** exponents[random_index]\n",
    "        random_params[\"msm\"] = {\"c\": c}\n",
    "\n",
    "        return random_params\n",
    "\n",
    "    def get_candidate_splitter(self, X, y):\n",
    "        \"\"\"Generate candidate splitter.\n",
    "\n",
    "        Takes a time series dataset and a set of parameterized\n",
    "        distance measures to create a candidate splitter, which\n",
    "        contains a parameterized distance measure and a set of exemplars.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray shape (n_cases, n_timepoints)\n",
    "            The training input samples.\n",
    "        y : np.array shape (n_cases,) or (n_cases,1)\n",
    "        parameterized_distances : dictionary\n",
    "            Contains the distances and their parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        splitter : list of two dictionaries\n",
    "            A distance and its parameter values and a set of exemplars.\n",
    "        \"\"\"\n",
    "        _X = X\n",
    "        _y = y\n",
    "\n",
    "        exemplars = {}\n",
    "        for label in np.unique(_y):\n",
    "            y_new = _y[_y == label]\n",
    "            X_new = _X[_y == label]\n",
    "            id = self.rng.randint(0, X_new.shape[0])\n",
    "            exemplars[y_new[id]] = X_new[id, :]\n",
    "\n",
    "        # Create a list with first element exemplars and second element a\n",
    "        # random parameterized distance measure\n",
    "        parameterized_distances = self.get_parameter_value(X)\n",
    "        n = self.rng.randint(0, 9)\n",
    "        dist = list(parameterized_distances.keys())[n]\n",
    "        splitter = [exemplars, {dist: parameterized_distances[dist]}]\n",
    "\n",
    "        return splitter\n",
    "\n",
    "    @staticmethod\n",
    "    def gini(y):\n",
    "        \"\"\"Get gini score at a specific node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : 1d numpy array\n",
    "            array of class labels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            gini score for the set of class labels (i.e. how pure they are). A\n",
    "            larger score means more impurity. Zero means\n",
    "            pure.\n",
    "        \"\"\"\n",
    "        # get number instances at node\n",
    "        n_instances = y.shape[0]\n",
    "        if n_instances > 0:\n",
    "            # count each class\n",
    "            unique_class_labels = np.unique(y)\n",
    "            # Count occurrences of each unique label using a loop\n",
    "            class_counts = np.array([np.sum(y == label) for label in unique_class_labels])\n",
    "            # subtract class entropy from current score for each class\n",
    "            class_counts = np.divide(class_counts, n_instances)\n",
    "            class_counts = np.power(class_counts, 2)\n",
    "            sum = np.sum(class_counts)\n",
    "            return 1 - sum\n",
    "        else:\n",
    "            # y is empty, therefore considered pure\n",
    "            raise ValueError(\"y empty\")\n",
    "\n",
    "    def gini_gain(self, y, y_subs):\n",
    "        \"\"\"Get gini score of a split, i.e. the gain from parent to children.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : 1d array\n",
    "            array of class labels at parent\n",
    "        y_subs : list of 1d array like\n",
    "            list of array of class labels, one array per child\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            gini score of the split from parent class labels to children. Note a\n",
    "            higher score means better gain,\n",
    "            i.e. a better split\n",
    "        \"\"\"\n",
    "        if y.ndim != 1:\n",
    "            raise ValueError()\n",
    "        # find number of instances overall\n",
    "        parent_n_instances = y.shape[0]\n",
    "        # if parent has no instances then is pure\n",
    "        if parent_n_instances == 0:\n",
    "            for child in y_subs:\n",
    "                if len(child) > 0:\n",
    "                    raise ValueError(\"children populated but parent empty\")\n",
    "            return 0.5\n",
    "        # find gini for parent node\n",
    "        score = self.gini(y)\n",
    "        # sum the children's gini scores\n",
    "        for index in range(len(y_subs)):\n",
    "            child_class_labels = y_subs[index]\n",
    "            # ignore empty children\n",
    "            if len(child_class_labels) > 0:\n",
    "                # find gini score for this child\n",
    "                child_score = self.gini(child_class_labels)\n",
    "                # weight score by proportion of instances at child compared to\n",
    "                # parent\n",
    "                child_size = len(child_class_labels)\n",
    "                child_score *= child_size / parent_n_instances\n",
    "                # add to cumulative sum\n",
    "                score -= child_score\n",
    "        return score\n",
    "\n",
    "    def _find_target_value(self, y):\n",
    "        \"\"\"Get the class label of highest frequency.\"\"\"\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        # Find the index of the maximum count\n",
    "        max_index = np.argmax(counts)\n",
    "        mode_value = unique[max_index]\n",
    "        return mode_value\n",
    "\n",
    "    def get_best_splitter(self, X, y):\n",
    "        max_gain = float(\"-inf\")\n",
    "        best_splitter = None\n",
    "        for _ in range(self.n_splitters):\n",
    "            splitter = self.get_candidate_splitter(X, y)\n",
    "            labels = list(splitter[0].keys())\n",
    "            measure = list(splitter[1].keys())[0]\n",
    "            y_subs = [[] for k in range(len(labels))]\n",
    "            for j in range(X.shape[0]):\n",
    "                min_dist = float(\"inf\")\n",
    "                sub = None\n",
    "                for k in range(len(labels)):\n",
    "                    dist = distance(\n",
    "                        X[j],\n",
    "                        splitter[0][labels[k]],\n",
    "                        metric=measure,\n",
    "                        kwargs=splitter[1][measure],\n",
    "                    )\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        sub = k\n",
    "                y_subs[sub].append(y[j])\n",
    "            y_subs = [np.array(ele) for ele in y_subs]\n",
    "            gini_index = self.gini_gain(y, y_subs)\n",
    "            if gini_index > max_gain:\n",
    "                max_gain = gini_index\n",
    "                best_splitter = splitter\n",
    "        return best_splitter\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        # Set the unique class labels\n",
    "        self.classes_ = list(np.unique(y))\n",
    "\n",
    "        self.root = self._build_tree(\n",
    "            X, y, depth=0, node_id=\"0\", parent_target_value=None\n",
    "        )\n",
    "        self._is_fitted = True\n",
    "\n",
    "    def _build_tree(self, X, y, depth, node_id, parent_target_value=None):\n",
    "\n",
    "        # If the data reaching the node is empty\n",
    "        if len(X) == 0:\n",
    "            leaf_label = parent_target_value\n",
    "            leaf_distribution = {}\n",
    "            leaf = Node(\n",
    "                node_id=node_id,\n",
    "                _is_leaf=True,\n",
    "                label=leaf_label,\n",
    "                class_distribution=leaf_distribution,\n",
    "            )\n",
    "\n",
    "        # Target value in current node\n",
    "        target_value = self._find_target_value(y)\n",
    "        class_distribution = {\n",
    "            label: count / len(y)\n",
    "            for label, count in zip(*np.unique(y, return_counts=True))\n",
    "        }\n",
    "\n",
    "        # If min sample splits is reached\n",
    "        if self.min_samples_split >= len(X):\n",
    "            leaf_label = target_value\n",
    "            leaf = Node(\n",
    "                node_id=node_id,\n",
    "                _is_leaf=True,\n",
    "                label=leaf_label,\n",
    "                class_distribution=class_distribution,\n",
    "            )\n",
    "\n",
    "        # If max depth is reached\n",
    "        if (self.max_depth is not None) and (depth >= self.max_depth):\n",
    "            leaf_label = target_value\n",
    "            leaf = Node(\n",
    "                node_id=node_id,\n",
    "                _is_leaf=True,\n",
    "                label=leaf_label,\n",
    "                class_distribution=class_distribution,\n",
    "            )\n",
    "\n",
    "        # Pure node\n",
    "        if len(self.classes_) == 1:\n",
    "            leaf_label = target_value\n",
    "            leaf = Node(\n",
    "                node_id=node_id,\n",
    "                _is_leaf=True,\n",
    "                label=leaf_label,\n",
    "                class_distribution=class_distribution,\n",
    "            )\n",
    "            return leaf\n",
    "\n",
    "        # Find the best splitter\n",
    "        splitter = self.get_best_splitter(X, y)\n",
    "\n",
    "        # Create root node\n",
    "        node = Node(node_id=node_id, _is_leaf=False, splitter=splitter)\n",
    "\n",
    "        # For each exemplar split the data\n",
    "        labels = list(splitter[0].keys())\n",
    "        measure = list(splitter[1].keys())[0]\n",
    "        X_child = [[] for _ in labels]\n",
    "        y_child = [[] for _ in labels]\n",
    "        for i in range(len(X)):\n",
    "            min_dist = np.inf\n",
    "            id = None\n",
    "            for j in range(len(labels)):\n",
    "                dist = distance(\n",
    "                    X[i],\n",
    "                    splitter[0][labels[j]],\n",
    "                    metric=measure,\n",
    "                    kwargs=splitter[1][measure],\n",
    "                )\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    id = j\n",
    "            X_child[id].append(X[i])\n",
    "            y_child[id].append(y[i])\n",
    "        X_child = [np.array(ele) for ele in X_child]\n",
    "        y_child = [np.array(ele) for ele in y_child]\n",
    "        # For each exemplar, create a branch\n",
    "        for i in range(len(labels)):\n",
    "            child_node_id = node_id + \".\" + str(i)\n",
    "            child_node = self._build_tree(\n",
    "                X_child[i],\n",
    "                y_child[i],\n",
    "                depth=depth + 1,\n",
    "                node_id=child_node_id,\n",
    "                parent_target_value=target_value,\n",
    "            )\n",
    "            node.children[labels[i]] = child_node\n",
    "\n",
    "        return node\n",
    "        \n",
    "    def _predict(self, X):\n",
    "        if not self._is_fitted:\n",
    "            raise NotFittedError(\n",
    "                f\"This instance of {self.__class__.__name__} has not \"\n",
    "                f\"been fitted yet; please call `fit` first.\"\n",
    "            )\n",
    "        probas = self._predict_proba(X)\n",
    "        predictions = np.argmax(probas, axis=1)\n",
    "        return np.array([self.classes_[pred] for pred in predictions])\n",
    "\n",
    "    def _predict_proba(self, X):\n",
    "        if not self._is_fitted:\n",
    "            raise NotFittedError(\n",
    "                f\"This instance of {self.__class__.__name__} has not \"\n",
    "                f\"been fitted yet; please call `fit` first.\"\n",
    "            )\n",
    "        # Get the unique class labels\n",
    "        classes = self.classes_\n",
    "        class_count = len(classes)\n",
    "        probas = []\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            # Classify the data point and find the leaf node\n",
    "            leaf_node = self._classify(self.root, X[i])\n",
    "            \n",
    "            # Create probability distribution based on class counts in the leaf node\n",
    "            proba = np.zeros(class_count)\n",
    "            for class_label, class_proba in leaf_node.class_distribution.items():\n",
    "                proba[classes.index(class_label)] = class_proba\n",
    "            probas.append(proba)\n",
    "\n",
    "        return np.array(probas)\n",
    "\n",
    "    def _classify(self, treenode, x):\n",
    "        # Classify one data point using the proximity tree\n",
    "        if treenode._is_leaf:\n",
    "            return treenode\n",
    "        else:\n",
    "            measure = list(treenode.splitter[1].keys())[0]\n",
    "            branches = list(treenode.splitter[0].keys())\n",
    "            min_dist = np.inf\n",
    "            id = None\n",
    "            for i in range(len(branches)):\n",
    "                dist = distance(\n",
    "                    x,\n",
    "                    treenode.splitter[0][branches[i]],\n",
    "                    metric=measure,\n",
    "                    kwargs=treenode.splitter[1][measure],\n",
    "                )\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    id = i\n",
    "            return self._classify(treenode.children[branches[id]], x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86bd9101-ea03-4c25-b939-c4e99a58a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ProximityTree(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd434c1-484f-4241-9af8-84c5df6a8b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.datasets import load_arrow_head\n",
    "X, y = load_arrow_head(return_type='numpy2d')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17252f1c-7a55-4797-be16-958677f50955",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf._fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "564f9164-c9e9-405d-8091-6f224fe2d6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'euclidean': {},\n",
       " 'dtw': {'window': 0.238},\n",
       " 'ddtw': {'window': 0.031},\n",
       " 'wdtw': {'g': 0.896},\n",
       " 'wddtw': {'g': 0.595},\n",
       " 'erp': {'g': 0.692},\n",
       " 'lcss': {'epsilon': 0.689, 'window': 0.075},\n",
       " 'twe': {'lmbda': 7, 'nu': 0.01},\n",
       " 'msm': {'c': 10.0}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = clf.get_parameter_value(X_train)\n",
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25eb66f2-c4ec-4459-be58-79d86b18a05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8490566037735849"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf._predict(X_test)\n",
    "score = accuracy_score(y_pred,y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6558856e-147f-4d00-bc40-a79309e2dc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_ = np.unique(y)\n",
    "len(classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae49940-e002-405e-a19a-9f22ddd88f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
